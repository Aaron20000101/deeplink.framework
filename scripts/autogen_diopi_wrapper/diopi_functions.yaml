- schema: "aten::add.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code: printf("[%s:%s:%d]:%s\n",__FILE__,__FUNCTION__,__LINE__,"autogened function");
  interface: diopiAddScalar(ctx, out, self, &other, &alpha)

- schema: "aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code: >
    printf("[%s:%s:%d]:%s\n",__FILE__,__FUNCTION__,__LINE__,__DATE__"autogened function");
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_add_scalar_out(self, other.item(), alpha, out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_add_scalar_out(other, self.item(), alpha, out);
    }
  interface: diopiAdd(ctx, out, self, other, &alpha)

- schema: "aten::sub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code: printf("[%s:%s:%d]:%s\n",__FILE__,__FUNCTION__,__LINE__,"autogened function");
  interface: diopiSubScalar(ctx, out, self, &other, &alpha)

- schema: "aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code: >
    printf("[%s:%s:%d]:%s\n",__FILE__,__FUNCTION__,__LINE__,"autogened function");
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_sub_scalar_out(self, other.item(), alpha, out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_sub_scalar_out(other, self.item(), alpha, out);
    }
  interface: diopiSub(ctx, out, self, other, &alpha)

- schema: "aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiDiv(&context, out, self, other, RoundModeNone)

- schema: "aten::div.Scalar_mode_out(Tensor self, Scalar other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)"
  custom_code: >
    const auto mode = toDiopiRoundMode(rounding_mode.has_value() ? rounding_mode.value().data():"none");
  interface: diopiDivScalar(&context, out, self, &other, mode)

- schema: "aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)"
  custom_code: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_div_scalar_mode_out(self, other.item(), rounding_mode, out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_div_scalar_mode_out(other, self.item(), rounding_mode, out);
    }
    const auto mode = toDiopiRoundMode(rounding_mode.has_value() ? rounding_mode.value().data():"none");
  interface: diopiDiv(&context, out, self, other, mode)

- schema: "aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)"
  interface: diopiFill(&context, self, &value)

- schema: "aten::mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  interface: diopiMul(&context, self, self, other)

#- schema: "aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
#  interface: diopiBatchNorm(&context, out, save_mean, save_invstd, input, weight, bias, const_cast<diopiTensorHandle_t>(running_mean), const_cast<diopiTensorHandle_t>(running_var), training, momentum, eps);

#- schema: "aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)"
#  custom_code: >
#    const int64_t dim_c = input.size(1);
#    auto out0 = at::empty_like(input);
#    auto options = input.options().dtype(at::kFloat);
#    auto out1 = at::empty({dim_c}, options);
#    auto out2 = at::empty({dim_c}, options);
#  interface: diopiBatchNorm(&context, out0, out1, out2, input, weight, bias, const_cast<diopiTensorHandle_t>(running_mean), const_cast<diopiTensorHandle_t>(running_var), training, momentum, eps);
